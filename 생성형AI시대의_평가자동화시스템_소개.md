## AI 시대의 공정한 프로그래밍 평가를 위한 종합 자동화 프레임워크 가이드

---

### **목차**

1.  **서론: 왜 새로운 평가 패러다임이 필요한가?**
2.  **시스템 개요: 과정 중심 평가의 완전한 자동화**
3.  **시험 절차 및 학생 가이드**
4.  **자동화된 평가 워크플로우: 신뢰와 검증의 파이프라인**
5.  **평가의 핵심: 동적 앙상블 시스템 'Inspector'**
6.  **결과 및 피드백: 데이터 기반의 교육적 대화**
7.  **시스템 구축 및 운영 가이드**
8.  **결론: 평가를 넘어 지속 가능한 성장으로**
9.  **부록 A: 관련 프로젝트 GitHub 저장소**
10. **참고문헌**

---

### 1. 서론: 왜 새로운 평가 패러다임이 필요한가?

지난 수십 년간 소프트웨어 개발 교육과 인재 채용의 현장에서 프로그래밍 역량을 측정하는 표준은 명확했다. 그것은 바로 최종적으로 제출된 코드, 즉 '결과물'의 완성도였다. 알고리즘의 효율성, 기능의 정확성, 코드의 가독성은 개발자의 실력을 가늠하는 신뢰할 수 있는 척도였다. 그러나 대규모 언어 모델(LLM)에 기반한 생성형 인공지능(AI)의 등장은 이 오랜 믿음의 근간을 흔들고 있다. GitHub Copilot, ChatGPT, AlphaCode 등은 이제 인간 전문가에 필적하는 코드를 단 몇 초 만에 생성하며, 개발의 패러다임을 송두리째 바꾸고 있다[1].

이러한 혁신은 생산성 향상이라는 빛나는 이면 뒤에, 평가의 공정성을 위협하는 짙은 그림자를 드리운다. 이제 최종 결과물만으로는 그것이 한 개인의 깊은 고민과 학습 과정의 산물인지, 아니면 AI가 생성한 결과물을 단순히 제출한 것인지 판별하는 것이 사실상 불가능해졌다. **"이 코드는 학생의 것인가, 기계의 것인가?"** 이 근본적인 질문 앞에서 전통적인 평가 방식은 무력해지며, 이는 교육의 본질인 '스스로 문제를 해결하는 능력의 함양'을 저해하는 심각한 도전으로 이어진다.

이러한 한계를 극복하기 위해, 평가의 패러다임은 개발 과정에서 생성되는 '디지털 발자취(Digital Footprint)'를 분석하는 **과정 중심 평가(Process-Oriented Evaluation)**로의 전환이 필연적이다[3, 4]. 버전 관리 시스템의 커밋 히스토리, 코드 변경 내역과 타임스탬프 등은 개발자의 사고 과정, 문제 해결 전략, 시행착오, 그리고 점진적인 개선 노력 등 AI가 완벽하게 모방하기 어려운 인간 고유의 특성을 담고 있는 풍부한 데이터 소스다. 본 프레임워크는 이러한 '과정'의 가치를 정량적, 정성적으로 분석하고 평가함으로써, AI 시대에 요구되는 새로운 평가의 표준을 제시하고자 한다. 우리의 목표는 단순한 감시를 넘어, 진정한 노력의 가치를 재발견하고 새로운 시대의 개발자 역량을 공정하게 평가할 새로운 나침반을 만드는 것이다.

### 2. 시스템 개요: 과정 중심 평가의 완전한 자동화

본 프레임워크는 과정 중심 평가의 복잡성을 해결하기 위해 설계된 통합 솔루션이다. 크게 **학생 측 환경**과 **평가자 측 환경**으로 나뉘며, '기록 → 암호화 → 제출 → 자동 분석 → 리포팅'의 전 과정을 유기적으로 연결한다.

#### **시스템 구성 요소**

| 구분             | 컴포넌트 (Component)                                                     | 역할 (Role)                                              |
| :--------------- | :----------------------------------------------------------------------- | :------------------------------------------------------- |
| **학생 측 환경**   | 🐍 [**`mission-python`**](https://github.com/drsungwon/mission-python)       | 개발 과정 자동 추적 및 암호화 시스템                     |
| **평가자 측 환경** | 🚀 [**`Orchestrator`**](https://github.com/drsungwon/orchestrator) (Python/Rust) | 전체 평가 파이프라인 자동화 지휘자 (Orchestrator)          |
|                  | 🔐 [**`mission-decoder`**](https://github.com/drsungwon/mission-decoder) (Python/Rust) | 암호화된 로그 및 서명 파일 복호화 도구                 |
|                  | 🦊 [**`mission-restore`**](https://github.com/drsungwon/mission-restore) (Python/Rust) | 개발 로그로부터 최종 소스 코드 복원 도구               |
|                  | 🔎 [**`duplicate_finder`**](https://github.com/drsungwon/duplicate_finder) | 제출물 간 내용 기반 중복 파일 탐지 도구                   |
|                  | 🔬 `loose-diff` / `ldiff-rs`                                             | 코드 간의 실질적 동일성 비교 검증 도구                 |
|                  | 🧠 [**`Inspector`**](https://github.com/drsungwon/inspector)              | 동적 앙상블 기반 개발 과정 심층 분석 및 평가 시스템 |

#### **전체 워크플로우**
 
1.  **기록 및 암호화 (학생):** 학생은 `mission-python` 환경에서 코드를 작성하고 실행한다. 이 과정에서 모든 코드 변경 이력과 시스템 환경 정보(위치, IP 등)가 자동으로 암호화되어 기록된다.
2.  **제출 (학생):** 학생은 시험 종료 후 `mission-python` 프로젝트 폴더 전체를 제출한다.
3.  **자동 평가 실행 (교수):** 교수는 제출된 모든 학생 폴더를 한곳에 모으고, `Orchestrator`를 단 한 번 실행한다.
4.  **분석 및 리포팅 (시스템):** `Orchestrator`는 모든 하위 도구들을 지휘하여 제출물을 병렬로 분석하고, 최종적으로 종합 평가 보고서(CSV)와 학생 개별 피드백 보고서(HTML)를 생성한다.

### 3. 시험 절차 및 학생 가이드

학생의 입장에서 본 프레임워크를 이용한 시험 응시는 매우 직관적이고 간단하다. 복잡한 설정 없이 문제 해결 본연의 과정에만 집중할 수 있도록 설계되었다.

#### **시험 절차**
1.  **환경 준비:** 제공된 [`mission-python`](https://github.com/drsungwon/mission-python) 프로젝트 폴더를 다운로드하고, `poetry install` 명령어로 필요한 라이브러리를 설치한다.
2.  **코드 작성:** `src/mission_python/main.py` 파일을 열어 주어진 문제에 대한 해결 코드를 작성한다. 학생은 이 **단 하나의 파일**에만 집중하면 된다.
3.  **실행 및 테스트:** 코드를 실행하고 테스트하려면 터미널에서 다음 명령어를 사용한다:
    ```bash
    poetry run python src/mission_python/main.py
    ```
    **이 `poetry run` 명령어를 실행할 때마다, 현재까지의 모든 코드 변경사항이 자동으로 기록된다는 점이 핵심이다.**
4.  **제출:** 시험 시간이 종료되면, 별도의 파일 정리 없이 프로젝트 폴더 전체를 그대로 압축하여 제출한다.

#### **유의사항 및 권장사항**
-   **꾸준한 실행이 중요합니다:** 완벽한 코드를 한 번에 작성하려 하지 말고, 기능 단위로 코드를 완성할 때마다 `poetry run`으로 실행하여 중간 과정을 꾸준히 기록하는 것이 좋다. 시스템은 이러한 점진적이고 꾸준한 개발 과정을 긍정적으로 평가한다.
-   **AI 활용은 현명하게:** 생성형 AI를 보조 도구로 활용하는 것은 권장된다. 하지만 AI가 생성한 코드를 그대로 복사하여 제출하는 행위는 과정 기록에 비정상적인 패턴(매우 짧은 시간에 방대한 코드 추가)을 남기며, 이는 부정행위로 간주되어 매우 낮은 점수를 받게 된다. AI의 제안을 참고하되, 이를 자신의 것으로 이해하고 수정하여 통합하는 과정을 보여주는 것이 중요하다.
-   **정직한 응시:** 시험 시작 시 최초 1회에 한해 응시 환경(IP 주소, 위치 등)이 기록된다. 대리 시험이나 허가되지 않은 장소에서의 응시는 자동으로 탐지된다.

### 4. 자동화된 평가 워크플로우: 신뢰와 검증의 파이프라인

교수가 학생들의 제출물을 받아 `Orchestrator`를 실행하면, 눈에 보이지 않는 곳에서 다음과 같은 정교하고 체계적인 검증 파이프라인이 작동한다. 이 모든 과정은 **'신뢰하되, 검증한다(Trust, but verify)'**는 원칙에 따라 설계되었다.

1.  **단계 0: 전역 중복 검사 (Global Duplication Check)**
    *   **도구:** `duplicate_finder`
    *   **목적:** 특정 학생이 다른 학생의 `log.encrypted` 파일을 그대로 복사하여 제출하는 가장 기본적인 부정행위를 사전에 차단한다.
    *   **과정:** `Orchestrator`는 개별 파일을 분석하기 전에, 모든 학생의 로그 파일 내용을 SHA-256 해시로 비교하여 100% 동일한 파일을 제출한 학생 그룹을 식별하고 최종 리포트에 기록한다.

2.  **단계 1: 데이터 해독 (Decryption)**
    *   **도구:** `mission-decoder`
    *   **목적:** 암호화된 학생들의 '디지털 증거'를 분석 가능한 상태로 만든다.
    *   **과정:** 평가자의 개인키(`private_key.pem`)를 사용하여, 각 학생의 `signature.encrypted`(시스템 서명)와 `log.encrypted`(개발 로그) 파일을 안전하게 복호화한다. 이 단계 없이는 어떠한 검증도 시작될 수 없다.

3.  **단계 2: 진실성 검증 (Authenticity Verification)**
    이 단계는 제출물의 근본적인 진실성을 여러 각도에서 교차 검증한다.

    *   **2-A. 응시 환경 검증 (Location Verification):**
        *   **대상:** 복호화된 `signature.json` 파일.
        *   **목적:** 대리 시험 및 지정 장소 이탈을 방지한다.
        *   **과정:** 파일 내의 IP 주소, IP 기반 위치 정보, MAC 주소 등을 분석하여 시험이 사전에 허가된 장소 및 네트워크 환경에서 수행되었는지 확인한다. 여러 학생의 제출물에서 동일한 시스템 정보가 발견될 경우, 이는 강력한 부정행위 정황으로 간주된다.

    *   **2-B. 과정-결과 일치도 검증 (Process-Product Consistency Check):**
        *   **도구:** `mission-restore`, `loose-diff`
        *   **목적:** 다른 사람의 완성된 답안(`main.py`)만 구해서 제출하고, 개발 과정 로그는 조작하거나 무시하는 행위를 방지한다.
        *   **과정:**
            1.  `mission-restore`가 복호화된 개발 로그(`log.decrypted`)의 'Initial Commit'과 모든 변경 이력(diff)을 순차적으로 적용하여, **로그에 기록된 과정만을 기반으로 한 최종 코드**를 복원한다.
            2.  `loose-diff`가 학생이 실제로 제출한 최종 코드(`main.py`)와 로그로부터 복원된 코드를 비교한다.
            3.  두 파일이 실질적으로 동일하면, 학생이 제출한 코드는 기록된 개발 과정의 정당한 결과물임이 증명된다. 만약 다르다면, 이는 기록된 과정과 무관한 외부 코드를 제출했음을 의미하는 명백한 증거가 된다.

4.  **단계 3: 심층 과정 분석 (In-depth Process Analysis)**
    *   **도구:** `Inspector`
    *   **목적:** 진실성이 검증된 개발 과정이 과연 '질적으로' 우수한지를 평가한다.
    *   **과정:** 복호화된 개발 로그와 총 시험 시간을 `Inspector`에 전달하여, 개발의 꾸준함, 체계성, AI 활용의 건전성 등을 종합적으로 분석하고 정량적인 점수를 산출한다.

### 5. 평가의 핵심: 동적 앙상블 시스템 'Inspector'

`Inspector`는 본 프레임워크의 '두뇌'이자, 평가의 정교함과 공정성을 담보하는 핵심 엔진이다. "모든 상황에 완벽한 단일 전문가는 없지만, 주어진 상황에 가장 적합한 전문가들로 위원회를 구성할 수는 있다"는 철학을 기반으로 한다.

#### **동적 앙상블 아키텍처**

`Inspector`는 단순히 하나의 알고리즘으로 로그를 평가하지 않는다. 3세대에 걸쳐 발전해 온 14개의 서로 다른 분석 모델(전문가)을 활용하며, 다음과 같은 지능적인 방식으로 작동한다.

1.  **개발 패턴 자동 분류 (Pattern Diagnosis):**
    분석을 시작하기 전, 로그의 전체적인 커밋 분포, 시간 간격 등 거시적 특징을 분석하여 학생의 개발 패턴을 다음 5가지 중 하나로 먼저 진단한다: `IDEAL_WORKER` (이상적), `PROCRASTINATOR` (벼락치기), `COPY_PASTE_HEAVY` (분할 복붙 의심), `CRAMMING_AI` (AI 답안 복붙), `DEFAULT` (일반).

2.  **동적 가중치 적용 (Dynamic Weighting):**
    진단된 패턴에 따라 각 전문가(모델)의 의견에 부여할 **가중치(중요도)**를 동적으로 조절한다.
    *   **'벼락치기'(`PROCRASTINATOR`)** 패턴이 감지되면, 시간 관리의 비효율성을 날카롭게 지적하는 시간 분석 전문가(DCA, CPA 모델 등)의 가중치를 높인다.
    *   **'AI 답안 복붙'(`CRAMMING_AI`)** 패턴이 의심되면, 명백한 규칙 위반을 잡아내는 강력한 게이트키퍼 모델(HAS, UPA-R)의 가중치를 극대화하여 변별력을 높인다.

3.  **집단 지성을 통한 최종 판정 (Collective Intelligence):**
    모든 전문가 모델을 병렬로 실행하여 각자의 점수를 계산하고, 동적으로 할당된 가중치를 적용하여 최종 앙상블 점수를 산출한다. 이를 통해 단일 모델의 편향이나 맹점을 극복하고, 다양한 관점을 종합한 훨씬 더 공정하고 신뢰할 수 있는 평가를 내릴 수 있다. 실험 결과, 이 방식은 기존 단일 모델 대비 정확도를 20-30% 향상시키고, 오탐률을 25-40% 감소시켰다.

**평가 신뢰성에 대한 답변:** 학생의 평가는 이처럼 고도로 정교하고 다각적인 분석을 통해 이루어진다. 단 하나의 규칙이나 모델이 아닌, 14명의 전문가로 구성된 위원회가 학생의 개발 패턴이라는 '상황'을 고려하여 내리는 종합적인 판단이므로, 그 결과를 신뢰할 수 있다.

### 6. 결과 및 피드백: 데이터 기반의 교육적 대화

본 프레임워크의 최종 목표는 단순한 점수 부여가 아닌, 학생의 성장을 유도하는 실질적인 피드백을 제공하는 것이다. 이를 위해 `Inspector`는 모든 분석 결과를 담은 상세한 **HTML 보고서**를 생성하며, 이는 학생에게 개별적으로 배포된다.

#### **HTML 보고서의 구성**
1.  **종합 분석 요약:** 최종 점수, 핵심 진단 패턴 등 가장 중요한 정보를 한눈에 보여준다.
2.  **전문가 평가 현황:** 최종 점수가 어떻게 산출되었는지, 14명의 전문가가 각각 어떤 점수와 의견을 냈는지 투명하게 공개한다.
3.  **주요 증거 및 해석:** 점수에 영향을 미친 긍정적/부정적 현상을 로그 기록에서 직접 찾아 "AI 생성 코드의 특징적 패턴 감지" 와 같이 구체적인 증거로 제시한다.
4.  **개선을 위한 권고사항:** 진단된 문제점을 개선하기 위한 구체적이고 실천 가능한 조언(Actionable Advice)을 제공한다. 예를 들어 'AI 복붙'이 감지된 학생에게는 "AI 제안 후 자신만의 해결책 개발, 과정 커밋 로그에 기록"과 같은 구체적인 실천 방안을 제시한다.

이 보고서는 교수가 학생의 개발 과정에 대한 질문에 답변할 때 강력한 근거 자료가 된다. "네 점수가 낮은 이유는, 보고서 3섹션의 증거에서 볼 수 있듯, 개발 시작 5분 만에 전체 코드의 80%가 추가되는 패턴이 여러 전문가 모델에 의해 'CRAMMING_AI'로 진단되었기 때문이야. 다음에는 4섹션의 권고사항처럼 문제를 작게 나누어 점진적으로 해결하는 과정을 보여주면 훨씬 좋은 평가를 받을 수 있을 거야"와 같이, **데이터에 기반한 구체적이고 건설적인 대화**가 가능해진다.

### 7. 시스템 구축 및 운영 가이드

이 강력한 평가 시스템을 실제 현장에서 운영하는 방법은 사용자의 역할에 따라 나뉜다.

#### **교수/조교 (일반 사용자)를 위한 가이드**
시스템 운영은 매우 간단하다. 다음의 폴더 구조만 준비되면 된다.
```
project-root/
├── student_submission/  # 여기에 모든 학생 폴더를 넣는다
├── tools/               # 여기에 모든 분석 도구와 키 파일을 넣는다
└── work/                # 여기서 Orchestrator를 실행한다
```
1.  **사전 준비:**
    *   제공된 `tools` 폴더를 준비한다. 여기에는 컴파일된 실행 파일(`duplicate_finder`, `inspector` 등)과 `mission-decoder.keys/private_key.pem` 개인키 파일이 포함되어 있어야 한다.
    *   학생들에게 받은 제출물 폴더들을 `student_submission` 폴더 안에 넣는다.
2.  **설치 (최초 1회):** `work` 폴더로 이동하여 터미널에 `poetry install`을 입력한다.
3.  **실행:** `work` 폴더에서 터미널에 `poetry run orchestrate --duration <시험시간(분)>`을 입력하면 모든 평가가 자동으로 진행된다.
4.  **결과 확인:** 실행이 완료되면 `work/output/` 폴더에 생성된 `evaluation_report.csv`와 학생별 HTML 보고서를 확인한다.

#### **기술 관리자/개발자를 위한 가이드**
시스템을 커스터마이징하거나 전문적으로 운영하기 위해서는 내부 구조에 대한 이해가 필요하다.
-   **유연한 환경 설정:** 모든 경로와 파일명은 `work/config.json` 파일에서 관리된다. 폴더 구조를 변경해야 할 경우, 코드 수정 없이 이 JSON 파일만 수정하면 된다.
-   **외부 도구 의존성:** `Orchestrator`는 `loose-diff`, `inspector` 등 일부 외부 도구의 표준 출력(stdout) 텍스트를 파싱하여 결과를 판단한다. (예: `inspector`의 출력에서 `"최종 앙상블 점수: (\d+)"` 패턴을 정규식으로 추출). 만약 이 도구들을 업데이트하여 출력 형식이 변경되면, `Orchestrator`의 파싱 로직도 함께 수정해야 한다.
-   **새로운 분석 모델 추가:** `Inspector`는 확장성을 고려하여 설계되었다. 새로운 분석 모델을 추가하려면, Rust의 `AnalysisModel` 트레이트(Trait)를 구현하는 새로운 모듈을 작성하고, `main.rs`의 모델 등록 리스트에 추가하면 된다. 이를 통해 프레임워크는 미래에 등장할 새로운 AI 탐지 기법이나 평가 철학을 지속적으로 흡수하며 발전할 수 있다.

### 8. 결론: 평가를 넘어 지속 가능한 성장으로

본 프레임워크는 생성형 AI 시대의 프로그래밍 평가가 마주한 복잡한 문제들에 대한 체계적이고 실용적인 해법을 제시한다. 학생의 개발 환경(`mission-python`)부터 평가자의 자동화된 분석 파이프라인(`Orchestrator`) 및 지능형 평가 엔진(`Inspector`)에 이르기까지, 전 과정은 **공정성, 신뢰성, 효율성, 그리고 교육적 가치**라는 핵심 목표를 달성하기 위해 유기적으로 설계되었다.

기술적 관점에서, 본 시스템은 **상황 인식(패턴 분류), 집단 지성(동적 앙상블), 그리고 완전한 투명성(해석 가능한 리포팅)**을 통해 기존 평가 방식의 한계를 극복했다. 실용적 관점에서, 이 모든 고도화된 분석을 단일 명령으로 자동화하여, 대규모 평가 환경에서도 손쉽게 적용할 수 있도록 만들었다.

궁극적으로 이 프레임워크는 단순한 평가 도구를 넘어, 학생에게는 자신의 성장 과정을 객관적으로 되돌아볼 수 있는 거울을 제공하고, 교수에게는 데이터에 기반한 효과적인 코칭을 가능하게 하는 강력한 교육 플랫폼이다. AI와 인간이 서로의 가치를 존중하며 함께 성장할 수 있는 건강한 개발 생태계를 구축하는 데, 본 연구가 의미 있는 첫걸음이 되기를 기대한다.

---

### 9. 부록 A: 관련 프로젝트 GitHub 저장소

-   **[mission-python]**: 학생들의 개발 과정을 자동으로 추적하고 암호화하는 Python 기반 시험 환경입니다.
-   **[Orchestrator]**: 복호화, 복원, 분석 등 전체 평가 파이프라인을 자동화하는 지휘자(Orchestrator)입니다.
-   **[mission-decoder]**: 암호화된 로그와 시스템 서명 파일을 복호화하는 유틸리티입니다.
-   **[mission-restore]**: 복호화된 개발 로그로부터 최종 소스 코드를 재구성하는 유틸리티입니다.
-   **[duplicate_finder]**: 제출된 파일들 간의 내용 기반 중복을 탐지하는 고성능 유틸리티입니다.
-   **[Inspector]**: 동적 앙상블 기법을 사용하여 개발 과정을 심층 분석하고 평가하는 핵심 분석 엔진입니다.

[mission-python]: https://github.com/drsungwon/mission-python
[Orchestrator]: https://github.com/drsungwon/orchestrator
[mission-decoder]: https://github.com/drsungwon/mission-decoder
[mission-restore]: https://github.com/drsungwon/mission-restore
[duplicate_finder]: https://github.com/drsungwon/duplicate_finder
[Inspector]: https://github.com/drsungwon/orchestrator/tree/main/**MISSION-INSPECTOR-OVERALL**

### 10. 참고문헌 (References)
[1] OpenAI, "GPT-4 Technical Report," *arXiv preprint arXiv:2303.08774*, 2023.
[2] P. Denny, V. Kumar, and N. Giacaman, "Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language," in *Proc. of the 54th ACM Technical Symposium on Computer Science Education (SIGCSE)*, 2023, pp. 1136-1142.
[3] A. Gitinabard, Y. Xu, et al., "How widely can analytical models predict student behavior? An analysis of the generalizability of process-based models," in *Proc. of the 9th International Conference on Learning Analytics & Knowledge (LAK)*, 2019, pp. 528-537.
[4] T. W. Price, Y. Dong, and T. Barnes, "Generating data-driven hints for open-ended programming," in *Proc. of the 9th International Conference on Educational Data Mining (EDM)*, 2016, pp. 191-198.
[5] P. Blikstein, "Using learning analytics to assess students’ behavior in open-ended programming tasks," in *Proc. of the 1st International Conference on Learning Analytics and Knowledge (LAK)*, 2011, pp. 110-116.